# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T43Y7Xoj2qBLv9vtvfRAXU2VgEPdMh6g

## trail classification
"""

import tensorflow as tf

print("TensorFlow version:", tf.__version__)

hello = tf.constant("Hello, TensorFlow!")

print(hello.numpy())

import numpy as np
import matplotlib.pyplot as plt

data=['good','great','well done','wonderful','bad','not good','dislike','useless']
label =[1,1,1,1,0,0,0,0]

one_hot =[tf.keras.preprocessing.text.one_hot(d,50) for d in data]
print(one_hot)

padding_of_x=tf.keras.preprocessing.sequence.pad_sequences(one_hot,padding='pre',maxlen=2)
print(padding_of_x)

emb_layer=tf.keras.layers.Embedding(50,4,input_length=4)
emb_layer(padding_of_x)

model=tf.keras.Sequential()
model.add(emb_layer)
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

model.summary()

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(tf.convert_to_tensor(padding_of_x), tf.convert_to_tensor(label), epochs=100,verbose=0,batch_size=2)

plt.plot(history.history['accuracy'])

def predict_word(word):
  one_hot_word=[tf.keras.preprocessing.text.one_hot(word,50)]
  padded_word=tf.keras.preprocessing.sequence.pad_sequences(one_hot_word,padding='pre',maxlen=2)
  result=model.predict(padded_word)
  if result[0][0]>0.5:
    print('positive')
  else:
    print('negative')

predict_word(word="good positive")

"""## Install tensorflow and pdfplumber"""

pip install tensorflow

pip install pdfplumber

"""## Text Classification

Classify the input reports to find the lab results (final results) or other. binary classification using tensorflow classification model as pagewise classification.

start with data processing page number with paragraph json.

then label the data for train dataset: if lab results or
(final results) in para then label as lab results and if not found label as others.

one hot encodding to convert word o ector representation

padding to bring all the text to scaled level

model train with first layer with embedding to give meaning to the input text
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pdfplumber

path =r'/content/AI_11_ISC_2_1.pdf'

"""### Text Extraction and labeled the data"""

def extract_text(pdf_path: str):
    results = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text() or ""
                results.append({
                    "page": page.page_number,
                    "para": page_text.replace("\n", " ").strip()
                })
        return results
    except Exception as e:
      print(e)

df=extract_text(path)
print(df)

import re

def label_text(data_list):
  labeled_data = []
  for item in data_list:
    page_no = item["page"]
    para_text = item["para"]
    para_text_lower = para_text.lower()

    if "lab results" in para_text_lower or "(final result)" in para_text_lower:
      labeled_data.append({"page": page_no, "text": para_text, "label": "yes"})
    else:
      labeled_data.append({"page": page_no, "text": para_text, "label": "others"})
  return labeled_data

le=label_text(df)
print(le)

from sklearn.model_selection import train_test_split
train, test = train_test_split(le, test_size=0.2, random_state=42)

f=[d['page'] for d in train]
print(f)

balance_train= [d for d in train if d['label'] == 'yes']
balance_train += [d for d in train if d['label'] == 'others'][:len(balance_train)]
print(balance_train)

"""one hot encodding"""

one_hot =[tf.keras.preprocessing.text.one_hot(d['text'],50) for d in balance_train]
print(one_hot)
label_str=[d['label'] for d in balance_train]
label=[1 if l == 'yes' else 0 for l in label_str]
print(label)

padding_of_x=tf.keras.preprocessing.sequence.pad_sequences(one_hot,maxlen=12,padding='post')
print(padding_of_x)

model=tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(input_dim=500,output_dim=64))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model.build(input_shape=(None, 12))
model.summary()

history=model.fit(tf.convert_to_tensor(padding_of_x), tf.convert_to_tensor(label), epochs=10,verbose=1,batch_size=2)

plt.plot(history.history['accuracy'])

plt.plot(history.history['loss'])

#predict_word="Lab Results Component Value Date HGBA1C 8.1 (H) 04/08/2024"

test_texts = [d['text'] for d in test]
test_labels = [d['label'] for d in test]
test_label = [1 if l == 'yes' else 0 for l in test_labels]
test_one_hot=[tf.keras.preprocessing.text.one_hot(text,50) for text in test_texts]
test_padding=tf.keras.preprocessing.sequence.pad_sequences(test_one_hot,padding='post',maxlen=12)
testing=model.predict(test_padding)

print(testing)
s=[d['label'] for d in test]
print(s)
result={"page":s,"label":[test for i,test in enumerate(testing)]}
print(result)

loss, accuracy = model.evaluate(test_padding, tf.convert_to_tensor(test_label))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""## synthetic data creation"""

pip install faker

import random
from faker import Faker

fake = Faker()

def generate_synthetic_data(num_records=10):
    lab_keywords = ["Lab Results","Lab Results","Lab Results","Lab Results","Lab Results","Lab Results","Lab Results","Lab Results","Lab Results"]

    data = []

    for i in range(1, num_records + 1):
        has_lab_results = random.choice([True, False])
        if has_lab_results:
            keyword = random.choice(lab_keywords)
            text = f"{fake.paragraph()} The {keyword} is {random.randint(50, 200)}."
            label = [1 if "lab results" in text.lower() else 0][0]
        else:
            text = fake.paragraph()
            label = [1 if "lab results" in text.lower() else 0][0]
        data.append({
            "pageno": i,
            "para": text,
            "label": label
        })

    return data

# Example usage
if __name__ == "__main__":
    synthetic_dataset = generate_synthetic_data(5000)
    for record in synthetic_dataset:
        print(record)

count = sum(record['label'] for record in synthetic_dataset)
print(count)

balance_train= [d for d in synthetic_dataset if d['label'] == 1]
balance_train += [d for d in synthetic_dataset if d['label'] == 0][:len(balance_train)]
count = sum(record['label'] for record in balance_train)
print(count)

train,test = train_test_split(balance_train,test_size=0.2,random_state=42)

balance_train = [d for d in train if d['label'] == 1]
balance_train += [d for d in train if d['label'] == 0][:len(balance_train)]
count = sum(record['label'] for record in balance_train)
print(count)

x_train, x_test, y_train, y_test=test,test,[d['label'] for d in test],[d['label'] for d in balance_train]

one_hot =[tf.keras.preprocessing.text.one_hot(d['para'],50) for d in x_train]
padding_of_x=tf.keras.preprocessing.sequence.pad_sequences(one_hot,maxlen=12,padding='post')

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(50, 64),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1, activation='sigmoid')
 ])
model.build(input_shape=(None, 12))

model.summary()

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) # Added model.compile()
history=model.fit(tf.convert_to_tensor(padding_of_x), tf.convert_to_tensor(y_train), epochs=12 ,validation_split= 0.2,verbose=1,batch_size=2)

plt.plot(history.history['accuracy'])

test_texts = [d['para'] for d in test]
test_labels = [d['label'] for d in test]
test_label = [1 if l == 'yes' else 0 for l in test_labels]
test_one_hot=[tf.keras.preprocessing.text.one_hot(text,50) for text in test_texts]
test_padding=tf.keras.preprocessing.sequence.pad_sequences(test_one_hot,padding='post',maxlen=12)

loss, accuracy = model.evaluate(test_padding, tf.convert_to_tensor(test_label))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""## Exploring model layers"""

import tensorflow as tf
from keras import layers

model1=tf.keras.models.Sequential([
    layers.Embedding(50,64),
    layers.Dense(32,activation='relu'),
    layers.Dense(16,activation='relu'),
    layers.Dense(8,activation='relu'),
    layers.Flatten(),
    layers.Dense(1,activation='sigmoid')
])
model1.build(input_shape=(None,12))
model1.summary()

model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

#model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) # Added model.compile()
history1=model1.fit(tf.convert_to_tensor(padding_of_x), tf.convert_to_tensor(y_train), epochs=12 ,validation_split= 0.2,verbose=1,batch_size=2)

plt.plot(history1.history['accuracy'])

loss, accuracy = model1.evaluate(test_padding, tf.convert_to_tensor(test_label))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

import tensorflow as tf
from keras import layers

model2=tf.keras.models.Sequential([
    layers.Embedding(50,128),
    layers.Dense(64,activation='relu'),
    layers.Dense(32,activation='relu'),
    layers.Dense(16,activation='relu'),
    layers.Dense(8,activation='relu'),
    layers.Flatten(),
    layers.Dense(1,activation='sigmoid')
])
model2.build(input_shape=(None,12))
model2.summary()

model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) # Added model.compile()
history=model2.fit(tf.convert_to_tensor(padding_of_x), tf.convert_to_tensor(y_train), epochs=12 ,validation_split= 0.2,verbose=1,batch_size=2)

plt.plot(history.history['accuracy'])

loss, accuracy = model2.evaluate(test_padding, tf.convert_to_tensor(test_label))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

model=tf.keras.models.Sequential([
    layers.Embedding(50,64),
    layers.Dense(32,activation='relu'),
    layers.MaxPooling1D(2),
    layers.Dense(16,activation='relu'),
    layers.Dense(8,activation='relu'),
    layers.Flatten(),
    layers.Dense(1,activation='sigmoid')
])
model.build(input_shape=(None,12))
model.summary()

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(tf.convert_to_tensor(padding_of_x), tf.convert_to_tensor(y_train), epochs=12 ,validation_split= 0.2,verbose=1,batch_size=2)

loss, accuracy = model2.evaluate(test_padding, tf.convert_to_tensor(test_label))
print(f"Test Accuracy: {accuracy * 100:.2f}%")



